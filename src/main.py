# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WjCGvx1MH51lP1iitT9gFo83D0Byg-z
"""

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

'''
    Method used to create Datasets for the models.
'''
def create_dataset(samples, features, classes, random_seed):
  dataset, labels = make_classification(n_samples=samples, n_features=features, n_informative=2, n_redundant=0, n_repeated=0, n_classes=classes, n_clusters_per_class=1, random_state=random_seed)
  data_train, data_test, labels_train, labels_test = train_test_split(dataset, labels, test_size=0.5, random_state=random_seed)

  return data_train, data_test, labels_train, labels_test

def calculate_generative_priors(labels):
  unique, counts = np.unique(labels, return_counts=True)
  prior_0 = counts[np.where(unique == 0)]/len(labels)
  prior_1 = counts[np.where(unique == 1)]/len(labels)
  prior_2 = counts[np.where(unique == 2)]/len(labels)
  
  return prior_0, prior_1, prior_2

def get_mean_stdev(dataset):
  return dataset.mean(0), dataset.std(0)

# Calculates the likelihood
def inference(x, mi, sigma):
  return (1 / np.sqrt(2 * np.pi) * sigma) * (np.exp((-(x - mi)**2) / (2 * sigma**2)))

def generative_model(train_dataset, train_label, test_dataset, test_label):
  # First, we calculate Priors for each class
  prior_0, prior_1, prior_2 = calculate_generative_priors(train_label)

  # After calculating Priors, we must calculate mean and standard deviation for each of the 3 classes
  mi_class_0, sigma_class_0 = get_mean_stdev(train_dataset[train_label == 0])
  mi_class_1, sigma_class_1 = get_mean_stdev(train_dataset[train_label == 1])
  mi_class_2, sigma_class_2 = get_mean_stdev(train_dataset[train_label == 2])
 ##  print ("train_dataset size: ", len(train_dataset))
 ## print ("train_label size: ", len(train_label))
 ## print ("test_dataset size: ", len(test_dataset))
 ## print ("test_label size: ", len(test_label))
 ## print ("prior size: ", len(prior_0))
 ## print ("training label size: ", len(train_label))
 ## print ("mi size: ", len(mi_class_0))
 ## print ("sigma size: ", len(sigma_class_2))
 ## print ("mi_0: ", mi_class_0)
 ##  print ("sigma_0: ", sigma_class_0)
 ##  print ("mi_1: ", mi_class_1)
 ##  print ("sigma_1: ", sigma_class_1)
  ##print ("mi_2: ", mi_class_2)
  ##print ("sigma_2: ", sigma_class_2)

  # Having this information, we can infere the probability for each class and check which one 
  # provides the maximum likelihood
  count = 0
  correct = 0
  for value in test_dataset:
    # Infer probablity of first feature for Class 0
   ## print ("value[0]: ", value[0])
    f1 = inference(value[0], mi_class_0[0], sigma_class_0[0])
    # Infer proability of second feature for class 0
    f2 = inference(value[1], mi_class_0[1], sigma_class_0[1])
   ## print ("f1 and f2 for class 0: ", f1, f2)
    # Calculate probability of class 0
    pc0 = f1*f2*prior_0
    #print ("f1: ", f1)
    #print ("f2: ", f2)
    # Infer probablity of first feature for Class 1
    f1 = inference(value[0], mi_class_1[0], sigma_class_1[0])
    # Infer proability of second feature for class 1
    f2 = inference(value[1], mi_class_1[1], sigma_class_1[1])
    # Calculate probability of class 1
    #print ("f1: ", f1)
    #print ("f2: ", f2)
    pc1 = f1*f2*prior_1

    # Infer probablity of first feature for Class 2
    f1 = inference(value[0], mi_class_2[0], sigma_class_2[0])
    # Infer proability of second feature for class 2
    f2 = inference(value[1], mi_class_2[1], sigma_class_2[1])
    # Calculate probability of class 2
    #print ("f1: ", f1)
    #print ("f2: ", f2)
    pc2 = f1*f2*prior_2

    list = (pc0, pc1, pc2)
    ##print ("list: ", pc0, pc1 ,pc2)
    if list.index(max(list)) == test_label[count]:
      correct += 1
    #else:
    #  print("Predicted", list.index(max(list)), " Label: ", test_label[count])
    count += 1
 ## print ("n of correct: ", correct)
 ## print ("n of test: ", len(test_label))
 ## print ("Accuracy: ", correct/len(test_label))

def discriminative_model():
  data_train, data_test, label_train, label_test = create_dataset(150, 2, 2, 42)
  

def main():

  #Create Dataset
  data_train_1, data_test_1, labels_train_1, labels_test_1 = create_dataset(150, 2, 2, 42)
  print ("dataset 01")
  print("data_train_1", data_train_1)
  print("data_test_1", data_test_1)
  print("labels_train_1", labels_train_1)
  print("labels_test_1", labels_test_1)
  print ("Running for the first dataset")
  ##generative_model(data_train_1, labels_train_1, data_test_1, labels_test_1)
  
  data_train_2, data_test_2, labels_train_2, labels_test_2 = create_dataset(150, 2, 2, 333)
  print ("dataset 02")
  print("data_train_2", data_train_2)
  print("data_test_2", data_test_2)
  print("labels_train_2", labels_train_2)
  print("labels_test_2", labels_test_2)
  ##print ("Running for the second dataset")
  ##generative_model(data_train_2, labels_train_2, data_test_2, labels_test_2)

  # Run discriminative model
  discriminative_model()
    

if __name__ == '__main__':
  main()
